{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from spamassassin_client import SpamAssassin\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from util import evaluate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n"
     ]
    }
   ],
   "source": [
    "with open('spammy_grams.txt', 'r') as f:\n",
    "    spammy_grams = f.read().splitlines()\n",
    "    spammy_grams = set([tuple(x.split(',')) for x in spammy_grams])\n",
    "print(len(spammy_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_spam_df = pd.read_csv('data/goodSpam/goodSpam.csv')\n",
    "good_spam_text = good_spam_df['text']\n",
    "good_spam_labels = good_spam_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just default to getting first synset of word, ordered by popularity, wont guarantee correct usage in context but should be good enough\n",
    "# could use BERT in this way as well? \"A synonym for ___ is ___\" prompt?\n",
    "# get nex\n",
    "# part of speech tagging to get more accurate synonyms?\n",
    "def replace_word(word):\n",
    "    try: \n",
    "        synset = wn.synsets(word)[0] # get most popular synset\n",
    "        lemmas = synset.lemmas()\n",
    "        for l in lemmas:\n",
    "            if word.lower() != l.name().lower():\n",
    "                return True, l.name().replace(\"_\", \" \")\n",
    "        return False, word\n",
    "    except:\n",
    "        return False, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_gram(gram):\n",
    "    new_gram = []\n",
    "    success = False\n",
    "    for w in gram:\n",
    "        s, w_new = replace_word(w) \n",
    "        if s:\n",
    "            success = True\n",
    "        new_gram.append(w_new)\n",
    "    return success, tuple(new_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_word(word):\n",
    "    return \"..\".join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subj_body_tokens(header, body):\n",
    "    # managing header\n",
    "    header_spl = header.split(\"\\n\", 1)\n",
    "    subj = header_spl[0]\n",
    "    header_leftovers = header_spl[1]\n",
    "    # ----\n",
    "    subj_content = subj.split(\"Subject:\", 1)[1].strip()\n",
    "    body_content = body.strip()\n",
    "\n",
    "    subj_tokens = nltk.word_tokenize(subj_content)\n",
    "    body_tokens = nltk.word_tokenize(body_content)\n",
    "\n",
    "    return subj_tokens, body_tokens, header_leftovers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconsruct_email(subj_tokens, header_leftovers, body_tokens):\n",
    "    header_str = \"Subject: \" + (TreebankWordDetokenizer().detokenize(subj_tokens).strip() + \"\\n\" + header_leftovers).strip()\n",
    "    body_str = TreebankWordDetokenizer().detokenize(body_tokens).strip()\n",
    "\n",
    "    return header_str + \"\\n\\n\" + body_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_to_tokens(ngram_list):\n",
    "    s = []\n",
    "    for i, ngram in enumerate(ngram_list):\n",
    "        if i == len(ngram_list) - 1:\n",
    "            s = s + list(ngram)\n",
    "        else:\n",
    "            s.append(ngram[0])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement_loop_syn(tokens, fallback_poison, n=2):\n",
    "    t = tokens\n",
    "    while n >= 1:\n",
    "        ngrams = list(nltk.ngrams(t, n))\n",
    "        new_ngrams = []\n",
    "        for gram in ngrams:\n",
    "            new_gram = gram\n",
    "            if gram in spammy_grams:\n",
    "                success, repl_gram = replace_gram(gram)\n",
    "                if success:\n",
    "                    new_gram = repl_gram\n",
    "                elif fallback_poison:\n",
    "                    new_gram = tuple(poison_word(w) for w in gram)\n",
    "            new_ngrams.append(new_gram)\n",
    "        t = ngrams_to_tokens(new_ngrams)\n",
    "        n -= 1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replace_attack(email, fallback_poison=False):\n",
    "    spl = email.split(\"\\n\\n\", 1)\n",
    "    header, body = spl[0].strip(), spl[1].strip()\n",
    "    subj_tokens, body_tokens, header_leftovers = get_subj_body_tokens(header, body)\n",
    "\n",
    "    new_subj_tokens = replacement_loop_syn(subj_tokens, fallback_poison=fallback_poison)\n",
    "    new_body_tokens = replacement_loop_syn(body_tokens, fallback_poison=fallback_poison)\n",
    "    \n",
    "    new_email = reconsruct_email(new_subj_tokens, header_leftovers, new_body_tokens)\n",
    "\n",
    "    return new_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacement_loop_poison(tokens, n=2):\n",
    "    t = tokens\n",
    "    while n >= 1:\n",
    "        ngrams = list(nltk.ngrams(t, n))\n",
    "        new_ngrams = []\n",
    "        for gram in ngrams:\n",
    "            new_gram = gram\n",
    "            if gram in spammy_grams:\n",
    "                new_gram = tuple(poison_word(w) for w in gram)\n",
    "            new_ngrams.append(new_gram)\n",
    "        t = ngrams_to_tokens(new_ngrams)\n",
    "        n -= 1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisoning_attack(email):\n",
    "    spl = email.split(\"\\n\\n\", 1)\n",
    "    header, body = spl[0].strip(), spl[1].strip()\n",
    "    subj_tokens, body_tokens, header_leftovers = get_subj_body_tokens(header, body)\n",
    "\n",
    "    new_subj_tokens = replacement_loop_poison(subj_tokens)\n",
    "    new_body_tokens = replacement_loop_poison(body_tokens)\n",
    "    \n",
    "    new_email = reconsruct_email(new_subj_tokens, header_leftovers, new_body_tokens)\n",
    "\n",
    "    return new_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executing the attack\n",
    "syn_repl_text = [synonym_replace_attack(t) for t in good_spam_text]\n",
    "poison_text = [poisoning_attack(t) for t in good_spam_text]\n",
    "both_att_text = [synonym_replace_attack(t, fallback_poison=True) for t in good_spam_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       268\n",
      "\n",
      "    accuracy                           1.00       268\n",
      "   macro avg       1.00      1.00      1.00       268\n",
      "weighted avg       1.00      1.00      1.00       268\n",
      "\n",
      "[[268]]\n",
      "AVG Score: 5.663805970149254\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline\n",
    "_, pred, scores = evaluate(zip(good_spam_text, good_spam_labels))\n",
    "# get avg score\n",
    "print(f'AVG Score: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.83      0.91       268\n",
      "\n",
      "    accuracy                           0.83       268\n",
      "   macro avg       0.50      0.42      0.45       268\n",
      "weighted avg       1.00      0.83      0.91       268\n",
      "\n",
      "[[  0   0]\n",
      " [ 45 223]]\n",
      "AVG Score: 4.967910447761194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickhansen/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nickhansen/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nickhansen/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# evaluate poison\n",
    "_, pred, scores = evaluate(zip(poison_text, good_spam_labels))\n",
    "print(f'AVG Score: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.82      0.90       268\n",
      "\n",
      "    accuracy                           0.82       268\n",
      "   macro avg       0.50      0.41      0.45       268\n",
      "weighted avg       1.00      0.82      0.90       268\n",
      "\n",
      "[[  0   0]\n",
      " [ 48 220]]\n",
      "AVG Score: 5.074253731343284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickhansen/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nickhansen/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nickhansen/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# evaluate synonym replacement\n",
    "_, pred, scores = evaluate(zip(syn_repl_text, good_spam_labels))\n",
    "print(f'AVG Score: {np.mean(scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate synonym replacement and poisoning fallback\n",
    "_, pred, scores = evaluate(zip(both_att_text, good_spam_labels))\n",
    "print(f'AVG Score: {np.mean(scores)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
