{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nickhansen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from spamassassin_client import SpamAssassin\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "nltk.download('stopwords')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 5000), (433, 2172), (347, 1891))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all of the formatted data\n",
    "enron_df, ling_df, sacorp_df = pd.read_csv('data/formattedData/enronFormatted.csv'), pd.read_csv('data/formattedData/lingFormatted.csv'), pd.read_csv('data/formattedData/SAcorpusFormatted.csv')\n",
    "\n",
    "enron_spam = enron_df[enron_df['label'] == 1]\n",
    "enron_ham = enron_df[enron_df['label'] == 0]\n",
    "\n",
    "ling_spam = ling_df[ling_df['label'] == 1]\n",
    "ling_ham = ling_df[ling_df['label'] == 0]\n",
    "\n",
    "sacorp_spam = sacorp_df[sacorp_df['label'] == 1]\n",
    "sacorp_ham = sacorp_df[sacorp_df['label'] == 0]\n",
    "\n",
    "(enron_spam.shape[0], enron_ham.shape[0]), (ling_spam.shape[0], ling_ham.shape[0]), (sacorp_spam.shape[0], sacorp_ham.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Size: (3560, 2)\n",
      "Sampled Size: (3560, 2)\n"
     ]
    }
   ],
   "source": [
    "sampleSize = 347\n",
    "\n",
    "enron_spam_sample = enron_spam.sample(1000)\n",
    "enron_ham_sample = enron_ham.sample(1000)\n",
    "\n",
    "ling_spam_sample = ling_spam.sample(433)\n",
    "ling_ham_sample = ling_ham.sample(433)\n",
    "\n",
    "sacorp_spam_sample = sacorp_spam.sample(347)\n",
    "sacorp_ham_sample = sacorp_ham.sample(347)\n",
    "\n",
    "# combine the datasets\n",
    "data = pd.concat([enron_spam_sample, enron_ham_sample, ling_spam_sample, ling_ham_sample, sacorp_spam_sample, sacorp_ham_sample]).sample(frac=1).reset_index(drop=True)\n",
    "print(f'Original Dataset Size: {data.shape}')\n",
    "data = data.sample(n=3560).reset_index(drop=True) # take random sample of n = 2000\n",
    "print(f'Sampled Size: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header(text):\n",
    "    # remove header and just append subject content with body to keep that semantic heading information\n",
    "    spl = text.split(\"\\n\\n\", 1)\n",
    "\n",
    "    subj_content = spl[0].split(\"\\n\", 1)[0].split(\"Subject: \", 1)[1].strip()\n",
    "    body_content = spl[1].strip()\n",
    "\n",
    "    return subj_content + \"\\n\" + body_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuples = list(zip(data['text'], data['label']))\n",
    "data_tuples = [(remove_header(text), label) for (text, label) in data_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_to_n_grams(words, n=2):\n",
    "    n_grams = []\n",
    "    for i in range(1, n+1):\n",
    "        n_grams.extend(nltk.ngrams(words, i))\n",
    "    return n_grams\n",
    "\n",
    "def is_alpha_ngram(ngram):\n",
    "    return all([w.isalpha() for w in ngram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "gram_vocab = set() # vocab composed of bigrams and unigrams\n",
    "for text, label in data_tuples:\n",
    "    words = [w.lower() for w in nltk.word_tokenize(text)]\n",
    "    one_two_grams = get_one_to_n_grams(words, n=2)\n",
    "    for g in one_two_grams:\n",
    "        if is_alpha_ngram(g):\n",
    "            gram_vocab.add(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(text):\n",
    "    words = [w.lower() for w in nltk.word_tokenize(text)]\n",
    "    one_two_grams = get_one_to_n_grams(words, n=1)\n",
    "    features = {}\n",
    "    for g in one_two_grams:\n",
    "        if g in gram_vocab:\n",
    "            features[g] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tuples = [(get_features(text), label) for (text, label) in data_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2374, 1186)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitPoint = len(feature_tuples) // 3\n",
    "train, test = feature_tuples[splitPoint:], feature_tuples[:splitPoint]\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9595278246205734\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "# now, it is tested on the test set and the accuracy reported\n",
    "print(\"Accuracy: \", nltk.classify.accuracy(classifier, test)) #nltk.classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "        ('linguistics',) = True                0 : 1      =     95.3 : 1.0\n",
      "                ('ect',) = True                0 : 1      =     82.5 : 1.0\n",
      "          ('forwarded',) = True                0 : 1      =     46.7 : 1.0\n",
      "              ('vince',) = True                0 : 1      =     45.5 : 1.0\n",
      "      ('advertisement',) = True                1 : 0      =     37.3 : 1.0\n",
      "             ('syntax',) = True                0 : 1      =     36.7 : 1.0\n",
      "            ('grammar',) = True                0 : 1      =     35.4 : 1.0\n",
      "           ('deadline',) = True                0 : 1      =     33.3 : 1.0\n",
      "           ('abstract',) = True                0 : 1      =     32.7 : 1.0\n",
      "           ('mailings',) = True                1 : 0      =     32.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features_in_list(classifier, n=10):\n",
    "    \"\"\"\n",
    "    Return a nested list of the \"most informative\" features \n",
    "    used by the classifier along with it's predominant labels\n",
    "    \"\"\"\n",
    "    cpdist = classifier._feature_probdist       # probability distribution for feature values given labels\n",
    "    feature_list = []\n",
    "    for (fname, fval) in classifier.most_informative_features(n):\n",
    "        def labelprob(l):\n",
    "            return cpdist[l, fname].prob(fval)\n",
    "        labels = sorted([l for l in classifier._labels if fval in cpdist[l, fname].samples()], \n",
    "                        key=labelprob)\n",
    "        feature_list.append([fname, labels[-1]])\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_features = [x for x in show_most_informative_features_in_list(classifier, n=500) if x[1] == 1]\n",
    "len(spam_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('advertisement',),\n",
       " ('mailings',),\n",
       " ('viagra',),\n",
       " ('earning',),\n",
       " ('mlm',),\n",
       " ('advertising',),\n",
       " ('featured',),\n",
       " ('php',),\n",
       " ('tips',),\n",
       " ('websites',)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammy_grams = [x[0] for x in spam_features]\n",
    "spammy_grams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the spammy words to a file\n",
    "with open('spammy_grams.txt', 'w') as f:\n",
    "    for gram in spammy_grams:\n",
    "        f.write(','.join(list(gram)) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
