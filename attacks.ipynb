{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from spamassassin_client import SpamAssassin\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from util import run_sa, evaluate\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('spammy_words.txt', 'r') as f:\n",
    "    spammy_words = set(f.read().splitlines())\n",
    "len(list(spammy_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just default to getting first synset of word, ordered by popularity, wont guarantee correct usage in context but should be good enough\n",
    "# could use BERT in this way as well? \"A synonym for ___ is ___\" prompt?\n",
    "# get nex\n",
    "def replace_word(word):\n",
    "    try: \n",
    "        synset = wn.synsets(word)[0] # get most popular synset\n",
    "        lemmas = synset.lemmas()\n",
    "        for l in lemmas:\n",
    "            if word.lower() != l.name().lower():\n",
    "                return True, l.name().replace(\"_\", \" \")\n",
    "        return False, word\n",
    "    except:\n",
    "        return False, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poison_word(word):\n",
    "    return \"..\".join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subj_body_tokens(header, body):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replace_attack(email):\n",
    "    spl = email.split(\"\\n\\n\", 1)\n",
    "    header, body = spl[0], spl[1]\n",
    "\n",
    "    header = spl[0] # first line in subject is always the subject \n",
    "    header_spl = header.split(\"\\n\", 1)\n",
    "    remain_header = header_spl[1]\n",
    "\n",
    "    subj = header_spl[0]\n",
    "\n",
    "    subj_content = subj.split(\"Subject: \", 1)[1].strip()\n",
    "    body_content = spl[1]\n",
    "\n",
    "    # TOKENIZE\n",
    "    subj_tokens = nltk.word_tokenize(subj_content)\n",
    "    body_tokens = nltk.word_tokenize(body_content)\n",
    "\n",
    "    new_subj_tokens = []\n",
    "    new_body_tokens = []\n",
    "\n",
    "    num_potential = 0 # we know there is a spammy word here\n",
    "    num_successful = 0 # we replaced it with something\n",
    "    repl = [] # replacement information\n",
    "    for s_t in subj_tokens:\n",
    "        w_new = s_t\n",
    "        if s_t in spammy_words:\n",
    "            num_potential += 1\n",
    "            success, new_word = replace_word(s_t)\n",
    "            w_new = new_word\n",
    "            if success:\n",
    "                num_successful += 1\n",
    "            repl.append((s_t, new_word, success))\n",
    "        new_subj_tokens.append(w_new)\n",
    "            \n",
    "\n",
    "    for b_t in body_tokens:\n",
    "        w_new = b_t\n",
    "        if b_t in spammy_words:\n",
    "            num_potential += 1\n",
    "            success, new_word = replace_word(b_t)\n",
    "            w_new = new_word\n",
    "            if success:\n",
    "                num_successful += 1\n",
    "            repl.append((b_t, new_word, success))\n",
    "        new_body_tokens.append(w_new)\n",
    "\n",
    "    new_header = \"Subject: \" + TreebankWordDetokenizer().detokenize(new_subj_tokens) + \"\\n\" + remain_header\n",
    "    new_body = TreebankWordDetokenizer().detokenize(new_body_tokens)\n",
    "    new_email = new_header + \"\\n\\n\" + new_body\n",
    "\n",
    "    return num_potential, num_successful, repl, new_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_spam_df = pd.read_csv('data/goodSpam/goodSpam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# executing the attack\n",
    "syn_repl_ret = [synonym_replace_attack(t) for t in good_spam_df['text']]\n",
    "# poison_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total emails:  268\n",
      "Potential Replacements (has at least one 'spammy' word):  261\n",
      "Had at least one successful replacement:  248\n"
     ]
    }
   ],
   "source": [
    "# how many were successful?\n",
    "pot_successes = sum([1 if t[0] > 0 else 0 for t in syn_repl_ret])\n",
    "successes = sum([1 if t[1] > 0 else 0 for t in syn_repl_ret])\n",
    "\n",
    "print(\"Total emails: \", len(good_spam_df))\n",
    "print(\"Potential Replacements (has at least one 'spammy' word): \", pot_successes)\n",
    "print(\"Had at least one successful replacement: \", successes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/268 2/268 3/268 4/268 5/268 6/268 7/268 8/268 9/268 10/268 11/268 12/268 13/268 14/268 15/268 16/268 17/268 18/268 19/268 20/268 21/268 22/268 23/268 24/268 25/268 26/268 27/268 28/268 29/268 30/268 31/268 32/268 33/268 34/268 35/268 36/268 37/268 38/268 39/268 40/268 41/268 42/268 43/268 44/268 45/268 46/268 47/268 48/268 49/268 50/268 51/268 52/268 53/268 54/268 55/268 56/268 57/268 58/268 59/268 60/268 61/268 62/268 63/268 64/268 65/268 66/268 67/268 68/268 69/268 70/268 71/268 72/268 73/268 74/268 75/268 76/268 77/268 78/268 79/268 80/268 81/268 82/268 83/268 84/268 85/268 86/268 87/268 88/268 89/268 90/268 91/268 92/268 93/268 94/268 95/268 96/268 97/268 98/268 99/268 100/268 101/268 102/268 103/268 104/268 105/268 106/268 107/268 108/268 109/268 110/268 111/268 112/268 113/268 114/268 115/268 116/268 117/268 118/268 119/268 120/268 121/268 122/268 123/268 124/268 125/268 126/268 127/268 128/268 129/268 130/268 131/268 132/268 133/268 134/268 135/268 136/268 137/268 138/268 139/268 140/268 141/268 142/268 143/268 144/268 145/268 146/268 147/268 148/268 149/268 150/268 151/268 152/268 153/268 154/268 155/268 156/268 157/268 158/268 159/268 160/268 161/268 162/268 163/268 164/268 165/268 166/268 167/268 168/268 169/268 170/268 171/268 172/268 173/268 174/268 175/268 176/268 177/268 178/268 179/268 180/268 181/268 182/268 183/268 184/268 185/268 186/268 187/268 188/268 189/268 190/268 191/268 192/268 193/268 194/268 195/268 196/268 197/268 198/268 199/268 200/268 201/268 202/268 203/268 204/268 205/268 206/268 207/268 208/268 209/268 210/268 211/268 212/268 213/268 214/268 215/268 216/268 217/268 218/268 219/268 220/268 221/268 222/268 223/268 224/268 225/268 226/268 227/268 228/268 229/268 230/268 231/268 232/268 233/268 234/268 235/268 236/268 237/268 238/268 239/268 240/268 241/268 242/268 243/268 244/268 245/268 246/268 247/268 248/268 249/268 250/268 251/268 252/268 253/268 254/268 255/268 256/268 257/268 258/268 259/268 260/268 261/268 262/268 263/268 264/268 265/268 266/268 267/268 268/268 \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.82      0.90       268\n",
      "\n",
      "    accuracy                           0.82       268\n",
      "   macro avg       0.50      0.41      0.45       268\n",
      "weighted avg       1.00      0.82      0.90       268\n",
      "\n",
      "[[  0   0]\n",
      " [ 49 219]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nickhansen/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nickhansen/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/nickhansen/miniconda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "og_spam_text = good_spam_df['text']\n",
    "new_spam_text = [t[3] for t in syn_repl_ret]\n",
    "spam_labels = good_spam_df['label']\n",
    "\n",
    "# evaluate\n",
    "spam_inputs = list(zip(new_spam_text, spam_labels))\n",
    "_ = evaluate(spam_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
