{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nickhansen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from spamassassin_client import SpamAssassin\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "nltk.download('stopwords')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 5000), (433, 2172), (347, 1891))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all of the formatted data\n",
    "enron_df, ling_df, sacorp_df = pd.read_csv('data/formattedData/enronFormatted.csv'), pd.read_csv('data/formattedData/lingFormatted.csv'), pd.read_csv('data/formattedData/SAcorpusFormatted.csv')\n",
    "\n",
    "enron_spam = enron_df[enron_df['label'] == 1]\n",
    "enron_ham = enron_df[enron_df['label'] == 0]\n",
    "\n",
    "ling_spam = ling_df[ling_df['label'] == 1]\n",
    "ling_ham = ling_df[ling_df['label'] == 0]\n",
    "\n",
    "sacorp_spam = sacorp_df[sacorp_df['label'] == 1]\n",
    "sacorp_ham = sacorp_df[sacorp_df['label'] == 0]\n",
    "\n",
    "(enron_spam.shape[0], enron_ham.shape[0]), (ling_spam.shape[0], ling_ham.shape[0]), (sacorp_spam.shape[0], sacorp_ham.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset Size: (3560, 2)\n",
      "Sampled Size: (3560, 2)\n"
     ]
    }
   ],
   "source": [
    "sampleSize = 347\n",
    "\n",
    "enron_spam_sample = enron_spam.sample(1000)\n",
    "enron_ham_sample = enron_ham.sample(1000)\n",
    "\n",
    "ling_spam_sample = ling_spam.sample(433)\n",
    "ling_ham_sample = ling_ham.sample(433)\n",
    "\n",
    "sacorp_spam_sample = sacorp_spam.sample(347)\n",
    "sacorp_ham_sample = sacorp_ham.sample(347)\n",
    "\n",
    "# combine the datasets\n",
    "data = pd.concat([enron_spam_sample, enron_ham_sample, ling_spam_sample, ling_ham_sample, sacorp_spam_sample, sacorp_ham_sample]).sample(frac=1).reset_index(drop=True)\n",
    "print(f'Original Dataset Size: {data.shape}')\n",
    "data = data.sample(n=3560).reset_index(drop=True) # take random sample of n = 2000\n",
    "print(f'Sampled Size: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header(text):\n",
    "    # remove header and just append subject content with body\n",
    "    spl = text.split(\"\\n\\n\", 1)\n",
    "\n",
    "    subj_content = spl[0].split(\"\\n\", 1)[0].split(\"Subject: \", 1)[1].strip()\n",
    "    body_content = spl[1].strip()\n",
    "\n",
    "    return subj_content + \"\\n\" + body_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuples = list(zip(data['text'], data['label']))\n",
    "data_tuples = [(remove_header(text), label) for (text, label) in data_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "vocab = set()\n",
    "for text, label in data_tuples:\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        w_l = word.lower()\n",
    "        if w_l.isalpha(): # TODO: should also check if in stopwords?\n",
    "            vocab.add(w_l)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(text):\n",
    "    rec_words = [w.lower() for w in nltk.word_tokenize(text)]\n",
    "    features = {}\n",
    "    for w in rec_words:\n",
    "        if w in vocab:\n",
    "            features[w] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_tuples = [(get_features(text), label) for (text, label) in data_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2374, 1186)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitPoint = len(feature_tuples) // 3\n",
    "train, test = feature_tuples[splitPoint:], feature_tuples[:splitPoint]\n",
    "len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9595278246205734\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train)\n",
    "# now, it is tested on the test set and the accuracy reported\n",
    "print(\"Accuracy: \", nltk.classify.accuracy(classifier, test)) #nltk.classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             linguistics = True                0 : 1      =     95.3 : 1.0\n",
      "                     ect = True                0 : 1      =     82.5 : 1.0\n",
      "               forwarded = True                0 : 1      =     46.7 : 1.0\n",
      "                   vince = True                0 : 1      =     45.5 : 1.0\n",
      "           advertisement = True                1 : 0      =     37.3 : 1.0\n",
      "                  syntax = True                0 : 1      =     36.7 : 1.0\n",
      "                 grammar = True                0 : 1      =     35.4 : 1.0\n",
      "                deadline = True                0 : 1      =     33.3 : 1.0\n",
      "                abstract = True                0 : 1      =     32.7 : 1.0\n",
      "                mailings = True                1 : 0      =     32.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_most_informative_features_in_list(classifier, n=10):\n",
    "    \"\"\"\n",
    "    Return a nested list of the \"most informative\" features \n",
    "    used by the classifier along with it's predominant labels\n",
    "    \"\"\"\n",
    "    cpdist = classifier._feature_probdist       # probability distribution for feature values given labels\n",
    "    feature_list = []\n",
    "    for (fname, fval) in classifier.most_informative_features(n):\n",
    "        def labelprob(l):\n",
    "            return cpdist[l, fname].prob(fval)\n",
    "        labels = sorted([l for l in classifier._labels if fval in cpdist[l, fname].samples()], \n",
    "                        key=labelprob)\n",
    "        feature_list.append([fname, labels[-1]])\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_features = [x for x in show_most_informative_features_in_list(classifier, n=500) if x[1] == 1]\n",
    "len(spam_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['advertisement',\n",
       " 'mailings',\n",
       " 'viagra',\n",
       " 'earning',\n",
       " 'mlm',\n",
       " 'advertising',\n",
       " 'featured',\n",
       " 'php',\n",
       " 'tips',\n",
       " 'websites']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammy_words = [x[0] for x in spam_features]\n",
    "spammy_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the spammy words to a file\n",
    "with open('spammy_words.txt', 'w') as f:\n",
    "    for word in spammy_words:\n",
    "        f.write(word + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
